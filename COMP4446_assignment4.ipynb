{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96b9cae-8123-4c55-a69a-8d9e051c76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a6e2c-e12a-4ab9-8aa0-e2f6ce660d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d6b23b-5c24-4ff9-a16e-5a06c71465cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class atisDataProcessor:\n",
    "    def __init__(self, data_file, type_file, glove_path, type_emb_dim=50, dtype_emb_dim=50):\n",
    "        # Tags Loading\n",
    "        # self.variable_names = []\n",
    "        self.var2idx = {}\n",
    "        self.idx2var = {}\n",
    "        self.var2idx[\"-\"] = 0 # Add additional variable \"-\" as other type\n",
    "        self.idx2var[0] = \"-\"\n",
    "        self.var_idx = 1\n",
    "        self.var2dtype = {}\n",
    "\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        # print(f\"{self.idx2var}\")\n",
    "        # print(f\"{self.var2idx}\")\n",
    "        self.name2idx = {} # tags mapping\n",
    "        self.idx2name = {}\n",
    "        self.name_idx = 2\n",
    "        self.name2idx[\"-\"] = 0\n",
    "        self.idx2name[0] = \"-\"\n",
    "        self.name2idx[\"PAD\"] = 1\n",
    "        self.idx2name[1] = \"PAD\"\n",
    "        \n",
    "        self.word2idx = {} # word mapping\n",
    "        self.idx2word = {}\n",
    "        self.word_idx = 1\n",
    "        self.word2idx[\"PAD\"] = 0\n",
    "        self.idx2word[0] = \"PAD\"\n",
    "        self.word2idx[\"UNK\"] = 1\n",
    "        self.idx2word[1] = \"UNK\"\n",
    "        \n",
    "        self.template2idx = {} # sql template mapping\n",
    "        self.idx2template = {}\n",
    "        self.template_idx = 0\n",
    "        self.var2dtype = {} # variable & datatype mapping\n",
    "        \n",
    "        self.train_data = [] # Training Dataset\n",
    "        self.dev_data = [] # Dev Dataset\n",
    "        self.test_data = [] # Testing Dataset\n",
    "\n",
    "        with open(type_file, 'r', encoding='utf-8') as tf:\n",
    "            print(f\"Loading datatype of variables for additional information on learning...\")\n",
    "            next(tf)\n",
    "            for line in tf:\n",
    "                parts = line.replace(\",\", \"\").strip().split()\n",
    "                self.var2dtype[parts[1].lower()] = parts[-1].lower()\n",
    "                # print(f\"Loading new datatype {parts[1].lower()} : {parts[-1].lower()}\")\n",
    "            type_set = sorted(set(self.var2dtype.values()))\n",
    "            self.dtype2idx = {t:i for i, t in enumerate(type_set)}\n",
    "            self.idx2dtype = {i:t for t, i in self.dtype2idx.items()}\n",
    "        # print(self.dtype2idx)\n",
    "        # print(self.var2dtype)\n",
    "        with open(data_file, 'r', encoding='utf-8') as df:\n",
    "            print(f\"Loading all data in json...\")\n",
    "            dataset = json.load(df)\n",
    "            print(f\"Loading sql template...\")\n",
    "            for obj in dataset:\n",
    "                template = min(obj['sql'], key=len)\n",
    "                template_with_default = []\n",
    "                template_with_default.append(template)\n",
    "                for var, value in obj['sentences'][0]['variables'].items():\n",
    "                    template_with_default.append({var: value})\n",
    "                if template not in self.template2idx:\n",
    "                    self.template2idx[template] = self.template_idx\n",
    "                    self.idx2template[self.template_idx] = template_with_default\n",
    "                    self.template_idx += 1\n",
    "                    print(f\"add a new template: {self.template_idx}\")\n",
    "                    # print(f\"{template_with_default}\")\n",
    "            print(len(self.template2idx))\n",
    "            self.template_classes = len(self.template2idx)\n",
    "            var_type = {}\n",
    "            print(f\"processing samples...\")\n",
    "            for obj in dataset:\n",
    "                # split = obj['query-split'] # split method for query split\n",
    "                for v in obj['variables']:\n",
    "                    var_type[v['name']] = v['type'].lower()\n",
    "                    if v['type'] not in self.var2idx:\n",
    "                        self.var2idx[v['type']] = self.var_idx\n",
    "                        self.idx2var[self.var_idx] = v['type']\n",
    "                        self.var_idx += 1\n",
    "                    if v['name'] not in self.name2idx:\n",
    "                        self.name2idx[v['name']] = self.name_idx\n",
    "                        self.idx2name[self.name_idx] = v['name']\n",
    "                        self.name_idx += 1\n",
    "                \n",
    "                for sentence in obj['sentences']:\n",
    "                    split = sentence['question-split'] # split method for question split\n",
    "                    for var in sentence['variables'].keys():\n",
    "                        self.nlp.tokenizer.add_special_case(var, [{ORTH: var}]) # add variable to special case preventing tokensisation \n",
    "                    text = sentence['text']\n",
    "                    doc = self.nlp(text)\n",
    "                    tokens = [tok.text.lower() for tok in doc]\n",
    "                    labels = [self.name2idx['-']] * len(tokens)\n",
    "                    types = [self.var2idx['-']] * len(tokens)\n",
    "                    dtypes = [self.dtype2idx[self.var2dtype['-']]] * len(tokens)\n",
    "                    for i, tok in enumerate(tokens):\n",
    "                        if tok in var_type and var_type[tok] in self.var2idx:\n",
    "                            labels[i] = self.name2idx[tok]\n",
    "                            dtypes[i] = self.dtype2idx[self.var2dtype[var_type[tok]]]\n",
    "                            types[i] = self.var2idx[var_type[tok]]\n",
    "                        tokens_sp = [sentence['variables'].get(tok, tok) for tok in tokens]\n",
    "                        template_id = self.template2idx[min(obj['sql'], key=len)]\n",
    "                        sample = {'tokens': tokens_sp, 'vars': labels, 'type':types, 'dtype': dtypes, 'template': template_id, 'split': split}\n",
    "                        # structure of samples:\n",
    "                        # tokens: texts with tokenisation(SpaCy) and word embedding(GloVe)\n",
    "                        # vars: tags of each word(default: '-') with name2idx mapping\n",
    "                        # types: type of each word(default: '-') with var2idx mapping\n",
    "                        # dtypes: datatype of each word(default: '-') with dtype2idx mapping for additional information support\n",
    "                        # template_id: SQL template of each text, as there is probably more than one template for a text, I store the (question, sql) template with full connection\n",
    "                        # split: reference by query-split/question split for dividing samples to diff datasets\n",
    "                        # print(f\"Add a new sample with {split}: {sample}\")\n",
    "                        if split == 'train':\n",
    "                            self.train_data.append(sample)\n",
    "                        elif split == 'dev':\n",
    "                            self.dev_data.append(sample)\n",
    "                        elif split == 'test':\n",
    "                            self.test_data.append(sample)\n",
    "                        else:\n",
    "                            print(f\"this sample not belongs to any dataset, adding it to training dataset..\")\n",
    "                            self.train_data.append(sample)\n",
    "            print(f\"length of training set: {len(self.train_data)}\")\n",
    "            print(f\"length of training set: {len(self.dev_data)}\")\n",
    "            print(f\"length of training set: {len(self.test_data)}\")\n",
    "        \n",
    "        \n",
    "        self.wordmapping()\n",
    "        self.glovemapping()\n",
    "\n",
    "\n",
    "    def wordmapping(self):\n",
    "        # traverse all samples to construct vocabulary graph and mapping to index\n",
    "        for sample in self.train_data:\n",
    "            for token in sample['tokens']:\n",
    "                if token not in self.word2idx:\n",
    "                    self.word2idx[token] = self.word_idx\n",
    "                    self.idx2word[self.word_idx] = token\n",
    "                    self.word_idx += 1\n",
    "                    # print(f\"add a new word: {token}\")\n",
    "\n",
    "    def glovemapping(self):\n",
    "        # using GloVe for embedding word vectors\n",
    "        glove_dict = {}\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == 0: dims = len(line.split()) - 1\n",
    "                parts = line.strip().split()\n",
    "                word = parts[0]\n",
    "                vec = torch.tensor([float(x) for x in parts[1:]], dtype=torch.float)\n",
    "                glove_dict[word] = vec\n",
    "        vocab_size = len(self.word2idx)\n",
    "\n",
    "        self.embedding_matrix = torch.randn(vocab_size, dims) * 0.1\n",
    "        self.embedding_matrix[0] = torch.zeros(dims)\n",
    "        for word, idx in self.word2idx.items():\n",
    "            if word in glove_dict:\n",
    "                self.embedding_matrix[idx] = glove_dict[word]\n",
    "        del glove_dict\n",
    "\n",
    "    def getDataLoader(self, split=\"train\", batch_size=32, shuffle=True):\n",
    "        # return specific dataloader\n",
    "        if split == \"train\":\n",
    "            dataset = TextDataset(self.train_data)\n",
    "        elif split == \"dev\":\n",
    "            dataset = TextDataset(self.dev_data)\n",
    "        elif split == \"test\":\n",
    "            dataset = TextDataset(self.test_data)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown split: {}\".format(split))\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=self.collate_fn)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_len = (max(len(sample[\"tokens\"]) for sample in batch))\n",
    "        word_idx = torch.zeros(batch_size, max_len, dtype=torch.long)  # word2idx[0] = PAD\n",
    "        label_idx = torch.full((batch_size, max_len), fill_value=-100, dtype=torch.long) # labels of each word\n",
    "        type_idx = torch.zeros(batch_size, max_len, dtype=torch.long)   # type of labels\n",
    "        dtype_idx = torch.zeros(batch_size, max_len, dtype=torch.long)  # datatype of types\n",
    "        class_labels = torch.zeros(batch_size, dtype=torch.long)        # SQL template of each sample\n",
    "        for i, sample in enumerate(batch):\n",
    "            seq_len = len(sample[\"tokens\"])\n",
    "            for j, token in enumerate(sample[\"tokens\"]):\n",
    "                word_idx[i, j] = self.word2idx.get(token, self.word2idx['UNK'])\n",
    "            label_idx[i, :seq_len] = torch.tensor(sample[\"vars\"], dtype=torch.long)\n",
    "            type_idx[i, :seq_len] = torch.tensor(sample[\"type\"], dtype=torch.long)\n",
    "            dtype_idx[i, :seq_len] = torch.tensor(sample[\"dtype\"], dtype=torch.long)\n",
    "            class_labels[i] = sample[\"template\"]\n",
    "        return word_idx, label_idx, type_idx, dtype_idx, class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b390ef-0ec2-4312-af0b-095ef3dd78f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9bd142-34a8-4cf6-a50e-8cad81707b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModels(nn.Module):\n",
    "    \"\"\"\n",
    "    models for classification task:\n",
    "    Linear:\n",
    "    FFN:\n",
    "    LSTM:\n",
    "    Transformer:\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, type_vocab_size, dtype_vocab_size, type_emb_dim=50, dtype_emb_dim=50, \n",
    "                 model_type=\"linear\", hidden_dim=128, template_classes=0, tag_classes=0, num_layers=1, nhead=4):\n",
    "        super(ClassificationModels, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        vocab_size, word_emb_dim = embedding_matrix.size()\n",
    "        self.word_emb = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n",
    "        self.type_emb = nn.Embedding(type_vocab_size, type_emb_dim, padding_idx=0)\n",
    "        self.dtype_emb = nn.Embedding(dtype_vocab_size, dtype_emb_dim, padding_idx=0)\n",
    "        input_dim = word_emb_dim + type_emb_dim + dtype_emb_dim\n",
    "        print(f\"Initialize {model_type} model...\")\n",
    "        if model_type == \"linear\":\n",
    "            self.fc_cls = nn.Linear(input_dim, template_classes)\n",
    "            self.fc_tag = nn.Linear(input_dim, tag_classes)\n",
    "        elif model_type == \"feedforward\":\n",
    "            self.ff_cls = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, template_classes)\n",
    "            )\n",
    "            self.ff_tag = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, tag_classes)\n",
    "            )\n",
    "        elif model_type == \"lstm\":\n",
    "            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers,\n",
    "                                  batch_first=True, bidirectional=True)\n",
    "            self.fc_cls = nn.Linear(hidden_dim*2, template_classes)\n",
    "            self.fc_tag = nn.Linear(hidden_dim*2, tag_classes)\n",
    "        elif model_type == \"transformer\":\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead)\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "            self.fc_cls = nn.Linear(input_dim, template_classes)\n",
    "            self.fc_tag = nn.Linear(input_dim, tag_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect model type\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, word_idx, type_idx, dtype_idx):\n",
    "        word_emb = self.word_emb(word_idx)        # [batch, seq_len, word_emb_dim]\n",
    "        type_emb = self.type_emb(type_idx)        # [batch, seq_len, type_emb_dim]\n",
    "        dtype_emb = self.dtype_emb(dtype_idx)     # [batch, seq_len, dtype_emb_dim]\n",
    "        x = torch.cat((word_emb, type_emb, dtype_emb), dim=2)  # [batch, seq_len, input_dim]\n",
    "        \n",
    "        if self.model_type == \"linear\":\n",
    "            cls_feat = x.mean(dim=1)  # [batch, input_dim]\n",
    "            class_logits = self.fc_cls(cls_feat)\n",
    "            tag_logits = self.fc_tag(x)  # [batch, seq_len, tag_classes]\n",
    "        elif self.model_type == \"feedforward\":\n",
    "            cls_feat = x.mean(dim=1)\n",
    "            class_logits = self.ff_cls(cls_feat)\n",
    "            tag_logits = self.ff_tag(x)\n",
    "        elif self.model_type == \"lstm\":\n",
    "            lstm_out, _ = self.lstm(x)  # [batch, seq_len, 2*hidden_dim]\n",
    "            cls_feat = lstm_out.mean(dim=1)\n",
    "            class_logits = self.fc_cls(cls_feat)\n",
    "            tag_logits = self.fc_tag(lstm_out)  # [batch, seq_len, tag_classes]\n",
    "        elif self.model_type == \"transformer\":\n",
    "            x_t = x.permute(1, 0, 2)  # [seq_len, batch, input_dim]\n",
    "            trans_out = self.transformer(x_t)  # [seq_len, batch, input_dim]\n",
    "            trans_out = trans_out.permute(1, 0, 2)  # [batch, seq_len, input_dim]\n",
    "            cls_feat = trans_out.mean(dim=1)\n",
    "            class_logits = self.fc_cls(cls_feat)\n",
    "            tag_logits = self.fc_tag(trans_out)\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect model type\")\n",
    "        return class_logits, tag_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca46ba19-0a71-4d64-9726-c51522defc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(processor, model, epochs=10, lr=1e-3, weight_cls=1.0, weight_tag=1.0, patience=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = \"cpu\"\n",
    "    model = model.to(device)\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_tag = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    best_val_acc = 0.0\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_corr_cls = 0\n",
    "        train_corr_tag = 0\n",
    "        train_total_cls = 0\n",
    "        train_total_tag = 0\n",
    "        train_loader = processor.getDataLoader(\"train\", shuffle=True)\n",
    "        for word_idx, label_idx, type_idx, dtype_idx, class_labels in train_loader:\n",
    "            word_idx = word_idx.to(device)\n",
    "            label_idx = label_idx.to(device)\n",
    "            type_idx = type_idx.to(device)\n",
    "            dtype_idx = dtype_idx.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            class_logits, tag_logits = model(word_idx, type_idx, dtype_idx)\n",
    "            # print(f\"class_logit: {class_logits}\")\n",
    "            # print(f\"class_labels: {class_labels}\")\n",
    "            # print(\"class_logits shape:\", class_logits.shape)      # [B, num_classes]\n",
    "            # print(\"class_labels max:\", class_labels.max().item())\n",
    "\n",
    "            loss_cls = criterion_cls(class_logits, class_labels)\n",
    "\n",
    "            loss_tag = criterion_tag(tag_logits.permute(0, 2, 1), label_idx)\n",
    "            loss = weight_cls * loss_cls + weight_tag * loss_tag\n",
    "            loss.backward()\n",
    "            torch.cuda.synchronize()\n",
    "            optimizer.step()\n",
    "            \n",
    "            preds = class_logits.argmax(dim=1)\n",
    "            train_corr_cls += (preds == class_labels).sum().item()\n",
    "            train_total_cls += class_labels.size(0)\n",
    "            pred_tags = tag_logits.argmax(dim=2)  # [batch, seq_len]\n",
    "            mask = (label_idx != -100)\n",
    "            train_corr_tag += ((pred_tags == label_idx) & mask).sum().item()\n",
    "            train_total_tag += mask.sum().item()\n",
    "        \n",
    "        train_acc_cls = train_corr_cls / train_total_cls if train_total_cls > 0 else 0\n",
    "        train_acc_tag = train_corr_tag / train_total_tag if train_total_tag > 0 else 0\n",
    "        \n",
    "        model.eval()\n",
    "        val_corr_cls = 0\n",
    "        val_corr_tag = 0\n",
    "        val_total_cls = 0\n",
    "        val_total_tag = 0\n",
    "        with torch.no_grad():\n",
    "            val_loader = processor.getDataLoader(\"dev\", shuffle=False)\n",
    "            for word_idx, label_idx, type_idx, dtype_idx, class_labels in val_loader:\n",
    "                word_idx = word_idx.to(device)\n",
    "                type_idx = type_idx.to(device)\n",
    "                dtype_idx = dtype_idx.to(device)\n",
    "                class_labels = class_labels.to(device)\n",
    "                label_idx = label_idx.to(device)\n",
    "                class_logits, tag_logits = model(word_idx, type_idx, dtype_idx)\n",
    "                \n",
    "                preds = class_logits.argmax(dim=1)\n",
    "                val_corr_cls += (preds == class_labels).sum().item()\n",
    "                val_total_cls += class_labels.size(0)\n",
    "                \n",
    "                pred_tags = tag_logits.argmax(dim=2)\n",
    "                mask = (label_idx != -100)\n",
    "                val_corr_tag += ((pred_tags == label_idx) & mask).sum().item()\n",
    "                val_total_tag += mask.sum().item()\n",
    "        val_acc_cls = val_corr_cls / val_total_cls if val_total_cls > 0 else 0\n",
    "        val_acc_tag = val_corr_tag / val_total_tag if val_total_tag > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train_cls_acc={train_acc_cls:.4f}, Train_tag_acc={train_acc_tag:.4f}, \" +\n",
    "              f\"Val_cls_acc={val_acc_cls:.4f}, Val_tag_acc={val_acc_tag:.4f}\")\n",
    "        \n",
    "        if val_acc_cls > best_val_acc:\n",
    "            best_val_acc = val_acc_cls\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"The accuracy of dev set seems not increase for 3 epoches, stop training...\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "503a7ee9-61fd-4a58-92af-305159e06e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(processor, model, batch_size=32):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    corr_cls = corr_tag = total_cls = total_tag = 0\n",
    "    loader = processor.getDataLoader(split=\"test\", batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for word_idx, label_idx, type_idx, dtype_idx, class_labels in loader:\n",
    "            word_idx = word_idx.to(device)\n",
    "            type_idx = type_idx.to(device)\n",
    "            dtype_idx = dtype_idx.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "            label_idx = label_idx.to(device)\n",
    "            class_logits, tag_logits = model(word_idx, type_idx, dtype_idx)\n",
    "\n",
    "            class_logits, tag_logits = model(word_idx, type_idx, dtype_idx)\n",
    "\n",
    "            preds = class_logits.argmax(dim=1)\n",
    "            corr_cls += (preds == class_labels).sum().item()\n",
    "            total_cls += class_labels.size(0)\n",
    "\n",
    "            pred_tags = tag_logits.argmax(dim=2)\n",
    "            mask = (label_idx != -100)\n",
    "            corr_tag += ((pred_tags == label_idx) & mask).sum().item()\n",
    "            total_tag += mask.sum().item()\n",
    "\n",
    "    acc_cls = corr_cls / total_cls if total_cls else 0\n",
    "    acc_tag = corr_tag / total_tag if total_tag else 0\n",
    "    return acc_cls, acc_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92308e76-d3e4-471c-9763-3af049e487bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, processor, question):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    tokens = [tok.text for tok in processor.nlp(question.strip())]\n",
    "    word_idxs = torch.tensor([[ processor.word2idx.get(tok.lower(), processor.word2idx[\"UNK\"]) \n",
    "                                 for tok in tokens ]], dtype=torch.long)\n",
    "    type_idxs = torch.zeros_like(word_idxs)\n",
    "    dtype_idxs = torch.zeros_like(word_idxs)\n",
    "    word_idxs = word_idxs.to(device)\n",
    "    type_idxs = type_idxs.to(device)\n",
    "    dtype_idxs = dtype_idxs.to(device)\n",
    "    with torch.no_grad():\n",
    "        class_logits, tag_logits = model(word_idxs, type_idxs, dtype_idxs)\n",
    "    pred_class = class_logits.argmax(dim=1).item()\n",
    "    pred_tags = tag_logits.argmax(dim=2).squeeze(0).tolist()  # [seq_len]\n",
    "    variables = []\n",
    "    for tok, tag in zip(tokens, pred_tags):\n",
    "        if tag != 0:\n",
    "            type_name = processor.idx2type.get(tag, \"UNK\")\n",
    "            variables.append((tok, type_name))\n",
    "    return pred_class, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d1c46d3-584a-4785-a650-a910d9a87727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationModels(nn.Module):\n",
    "    \"\"\"\n",
    "    models for generation task:\n",
    "    LSTM:\n",
    "    LSTM(With Attention):\n",
    "    Transformer:\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1910a588-994c-419e-9c13-984857207b02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datatype of variables for additional information on learning...\n",
      "Loading all data in json...\n",
      "Loading sql template...\n",
      "add a new template: 1\n",
      "add a new template: 2\n",
      "add a new template: 3\n",
      "add a new template: 4\n",
      "add a new template: 5\n",
      "add a new template: 6\n",
      "add a new template: 7\n",
      "add a new template: 8\n",
      "add a new template: 9\n",
      "add a new template: 10\n",
      "add a new template: 11\n",
      "add a new template: 12\n",
      "add a new template: 13\n",
      "add a new template: 14\n",
      "add a new template: 15\n",
      "add a new template: 16\n",
      "add a new template: 17\n",
      "add a new template: 18\n",
      "add a new template: 19\n",
      "add a new template: 20\n",
      "add a new template: 21\n",
      "add a new template: 22\n",
      "add a new template: 23\n",
      "add a new template: 24\n",
      "add a new template: 25\n",
      "add a new template: 26\n",
      "add a new template: 27\n",
      "add a new template: 28\n",
      "add a new template: 29\n",
      "add a new template: 30\n",
      "add a new template: 31\n",
      "add a new template: 32\n",
      "add a new template: 33\n",
      "add a new template: 34\n",
      "add a new template: 35\n",
      "add a new template: 36\n",
      "add a new template: 37\n",
      "add a new template: 38\n",
      "add a new template: 39\n",
      "add a new template: 40\n",
      "add a new template: 41\n",
      "add a new template: 42\n",
      "add a new template: 43\n",
      "add a new template: 44\n",
      "add a new template: 45\n",
      "add a new template: 46\n",
      "add a new template: 47\n",
      "add a new template: 48\n",
      "add a new template: 49\n",
      "add a new template: 50\n",
      "add a new template: 51\n",
      "add a new template: 52\n",
      "add a new template: 53\n",
      "add a new template: 54\n",
      "add a new template: 55\n",
      "add a new template: 56\n",
      "add a new template: 57\n",
      "add a new template: 58\n",
      "add a new template: 59\n",
      "add a new template: 60\n",
      "add a new template: 61\n",
      "add a new template: 62\n",
      "add a new template: 63\n",
      "add a new template: 64\n",
      "add a new template: 65\n",
      "add a new template: 66\n",
      "add a new template: 67\n",
      "add a new template: 68\n",
      "add a new template: 69\n",
      "add a new template: 70\n",
      "add a new template: 71\n",
      "add a new template: 72\n",
      "add a new template: 73\n",
      "add a new template: 74\n",
      "add a new template: 75\n",
      "add a new template: 76\n",
      "add a new template: 77\n",
      "add a new template: 78\n",
      "add a new template: 79\n",
      "add a new template: 80\n",
      "add a new template: 81\n",
      "add a new template: 82\n",
      "add a new template: 83\n",
      "add a new template: 84\n",
      "add a new template: 85\n",
      "add a new template: 86\n",
      "add a new template: 87\n",
      "add a new template: 88\n",
      "add a new template: 89\n",
      "add a new template: 90\n",
      "add a new template: 91\n",
      "add a new template: 92\n",
      "add a new template: 93\n",
      "add a new template: 94\n",
      "add a new template: 95\n",
      "add a new template: 96\n",
      "add a new template: 97\n",
      "add a new template: 98\n",
      "add a new template: 99\n",
      "add a new template: 100\n",
      "add a new template: 101\n",
      "add a new template: 102\n",
      "add a new template: 103\n",
      "add a new template: 104\n",
      "add a new template: 105\n",
      "add a new template: 106\n",
      "add a new template: 107\n",
      "add a new template: 108\n",
      "add a new template: 109\n",
      "add a new template: 110\n",
      "add a new template: 111\n",
      "add a new template: 112\n",
      "add a new template: 113\n",
      "add a new template: 114\n",
      "add a new template: 115\n",
      "add a new template: 116\n",
      "add a new template: 117\n",
      "add a new template: 118\n",
      "add a new template: 119\n",
      "add a new template: 120\n",
      "add a new template: 121\n",
      "add a new template: 122\n",
      "add a new template: 123\n",
      "add a new template: 124\n",
      "add a new template: 125\n",
      "add a new template: 126\n",
      "add a new template: 127\n",
      "add a new template: 128\n",
      "add a new template: 129\n",
      "add a new template: 130\n",
      "add a new template: 131\n",
      "add a new template: 132\n",
      "add a new template: 133\n",
      "add a new template: 134\n",
      "add a new template: 135\n",
      "add a new template: 136\n",
      "add a new template: 137\n",
      "add a new template: 138\n",
      "add a new template: 139\n",
      "add a new template: 140\n",
      "add a new template: 141\n",
      "add a new template: 142\n",
      "add a new template: 143\n",
      "add a new template: 144\n",
      "add a new template: 145\n",
      "add a new template: 146\n",
      "add a new template: 147\n",
      "add a new template: 148\n",
      "add a new template: 149\n",
      "add a new template: 150\n",
      "add a new template: 151\n",
      "add a new template: 152\n",
      "add a new template: 153\n",
      "add a new template: 154\n",
      "add a new template: 155\n",
      "add a new template: 156\n",
      "add a new template: 157\n",
      "add a new template: 158\n",
      "add a new template: 159\n",
      "add a new template: 160\n",
      "add a new template: 161\n",
      "add a new template: 162\n",
      "add a new template: 163\n",
      "add a new template: 164\n",
      "add a new template: 165\n",
      "add a new template: 166\n",
      "add a new template: 167\n",
      "add a new template: 168\n",
      "add a new template: 169\n",
      "add a new template: 170\n",
      "add a new template: 171\n",
      "add a new template: 172\n",
      "add a new template: 173\n",
      "add a new template: 174\n",
      "add a new template: 175\n",
      "add a new template: 176\n",
      "add a new template: 177\n",
      "add a new template: 178\n",
      "add a new template: 179\n",
      "add a new template: 180\n",
      "add a new template: 181\n",
      "add a new template: 182\n",
      "add a new template: 183\n",
      "add a new template: 184\n",
      "add a new template: 185\n",
      "add a new template: 186\n",
      "add a new template: 187\n",
      "add a new template: 188\n",
      "add a new template: 189\n",
      "add a new template: 190\n",
      "add a new template: 191\n",
      "add a new template: 192\n",
      "add a new template: 193\n",
      "add a new template: 194\n",
      "add a new template: 195\n",
      "add a new template: 196\n",
      "add a new template: 197\n",
      "add a new template: 198\n",
      "add a new template: 199\n",
      "add a new template: 200\n",
      "add a new template: 201\n",
      "add a new template: 202\n",
      "add a new template: 203\n",
      "add a new template: 204\n",
      "add a new template: 205\n",
      "add a new template: 206\n",
      "add a new template: 207\n",
      "add a new template: 208\n",
      "add a new template: 209\n",
      "add a new template: 210\n",
      "add a new template: 211\n",
      "add a new template: 212\n",
      "add a new template: 213\n",
      "add a new template: 214\n",
      "add a new template: 215\n",
      "add a new template: 216\n",
      "add a new template: 217\n",
      "add a new template: 218\n",
      "add a new template: 219\n",
      "add a new template: 220\n",
      "add a new template: 221\n",
      "add a new template: 222\n",
      "add a new template: 223\n",
      "add a new template: 224\n",
      "add a new template: 225\n",
      "add a new template: 226\n",
      "add a new template: 227\n",
      "add a new template: 228\n",
      "add a new template: 229\n",
      "add a new template: 230\n",
      "add a new template: 231\n",
      "add a new template: 232\n",
      "add a new template: 233\n",
      "add a new template: 234\n",
      "add a new template: 235\n",
      "add a new template: 236\n",
      "add a new template: 237\n",
      "add a new template: 238\n",
      "add a new template: 239\n",
      "add a new template: 240\n",
      "add a new template: 241\n",
      "add a new template: 242\n",
      "add a new template: 243\n",
      "add a new template: 244\n",
      "add a new template: 245\n",
      "add a new template: 246\n",
      "add a new template: 247\n",
      "add a new template: 248\n",
      "add a new template: 249\n",
      "add a new template: 250\n",
      "add a new template: 251\n",
      "add a new template: 252\n",
      "add a new template: 253\n",
      "add a new template: 254\n",
      "add a new template: 255\n",
      "add a new template: 256\n",
      "add a new template: 257\n",
      "add a new template: 258\n",
      "add a new template: 259\n",
      "add a new template: 260\n",
      "add a new template: 261\n",
      "add a new template: 262\n",
      "add a new template: 263\n",
      "add a new template: 264\n",
      "add a new template: 265\n",
      "add a new template: 266\n",
      "add a new template: 267\n",
      "add a new template: 268\n",
      "add a new template: 269\n",
      "add a new template: 270\n",
      "add a new template: 271\n",
      "add a new template: 272\n",
      "add a new template: 273\n",
      "add a new template: 274\n",
      "add a new template: 275\n",
      "add a new template: 276\n",
      "add a new template: 277\n",
      "add a new template: 278\n",
      "add a new template: 279\n",
      "add a new template: 280\n",
      "add a new template: 281\n",
      "add a new template: 282\n",
      "add a new template: 283\n",
      "add a new template: 284\n",
      "add a new template: 285\n",
      "add a new template: 286\n",
      "add a new template: 287\n",
      "add a new template: 288\n",
      "add a new template: 289\n",
      "add a new template: 290\n",
      "add a new template: 291\n",
      "add a new template: 292\n",
      "add a new template: 293\n",
      "add a new template: 294\n",
      "add a new template: 295\n",
      "add a new template: 296\n",
      "add a new template: 297\n",
      "add a new template: 298\n",
      "add a new template: 299\n",
      "add a new template: 300\n",
      "add a new template: 301\n",
      "add a new template: 302\n",
      "add a new template: 303\n",
      "add a new template: 304\n",
      "add a new template: 305\n",
      "add a new template: 306\n",
      "add a new template: 307\n",
      "add a new template: 308\n",
      "add a new template: 309\n",
      "add a new template: 310\n",
      "add a new template: 311\n",
      "add a new template: 312\n",
      "add a new template: 313\n",
      "add a new template: 314\n",
      "add a new template: 315\n",
      "add a new template: 316\n",
      "add a new template: 317\n",
      "add a new template: 318\n",
      "add a new template: 319\n",
      "add a new template: 320\n",
      "add a new template: 321\n",
      "add a new template: 322\n",
      "add a new template: 323\n",
      "add a new template: 324\n",
      "add a new template: 325\n",
      "add a new template: 326\n",
      "add a new template: 327\n",
      "add a new template: 328\n",
      "add a new template: 329\n",
      "add a new template: 330\n",
      "add a new template: 331\n",
      "add a new template: 332\n",
      "add a new template: 333\n",
      "add a new template: 334\n",
      "add a new template: 335\n",
      "add a new template: 336\n",
      "add a new template: 337\n",
      "add a new template: 338\n",
      "add a new template: 339\n",
      "add a new template: 340\n",
      "add a new template: 341\n",
      "add a new template: 342\n",
      "add a new template: 343\n",
      "add a new template: 344\n",
      "add a new template: 345\n",
      "add a new template: 346\n",
      "add a new template: 347\n",
      "add a new template: 348\n",
      "add a new template: 349\n",
      "add a new template: 350\n",
      "add a new template: 351\n",
      "add a new template: 352\n",
      "add a new template: 353\n",
      "add a new template: 354\n",
      "add a new template: 355\n",
      "add a new template: 356\n",
      "add a new template: 357\n",
      "add a new template: 358\n",
      "add a new template: 359\n",
      "add a new template: 360\n",
      "add a new template: 361\n",
      "add a new template: 362\n",
      "add a new template: 363\n",
      "add a new template: 364\n",
      "add a new template: 365\n",
      "add a new template: 366\n",
      "add a new template: 367\n",
      "add a new template: 368\n",
      "add a new template: 369\n",
      "add a new template: 370\n",
      "add a new template: 371\n",
      "add a new template: 372\n",
      "add a new template: 373\n",
      "add a new template: 374\n",
      "add a new template: 375\n",
      "add a new template: 376\n",
      "add a new template: 377\n",
      "add a new template: 378\n",
      "add a new template: 379\n",
      "add a new template: 380\n",
      "add a new template: 381\n",
      "add a new template: 382\n",
      "add a new template: 383\n",
      "add a new template: 384\n",
      "add a new template: 385\n",
      "add a new template: 386\n",
      "add a new template: 387\n",
      "add a new template: 388\n",
      "add a new template: 389\n",
      "add a new template: 390\n",
      "add a new template: 391\n",
      "add a new template: 392\n",
      "add a new template: 393\n",
      "add a new template: 394\n",
      "add a new template: 395\n",
      "add a new template: 396\n",
      "add a new template: 397\n",
      "add a new template: 398\n",
      "add a new template: 399\n",
      "add a new template: 400\n",
      "add a new template: 401\n",
      "add a new template: 402\n",
      "add a new template: 403\n",
      "add a new template: 404\n",
      "add a new template: 405\n",
      "add a new template: 406\n",
      "add a new template: 407\n",
      "add a new template: 408\n",
      "add a new template: 409\n",
      "add a new template: 410\n",
      "add a new template: 411\n",
      "add a new template: 412\n",
      "add a new template: 413\n",
      "add a new template: 414\n",
      "add a new template: 415\n",
      "add a new template: 416\n",
      "add a new template: 417\n",
      "add a new template: 418\n",
      "add a new template: 419\n",
      "add a new template: 420\n",
      "add a new template: 421\n",
      "add a new template: 422\n",
      "add a new template: 423\n",
      "add a new template: 424\n",
      "add a new template: 425\n",
      "add a new template: 426\n",
      "add a new template: 427\n",
      "add a new template: 428\n",
      "add a new template: 429\n",
      "add a new template: 430\n",
      "add a new template: 431\n",
      "add a new template: 432\n",
      "add a new template: 433\n",
      "add a new template: 434\n",
      "add a new template: 435\n",
      "add a new template: 436\n",
      "add a new template: 437\n",
      "add a new template: 438\n",
      "add a new template: 439\n",
      "add a new template: 440\n",
      "add a new template: 441\n",
      "add a new template: 442\n",
      "add a new template: 443\n",
      "add a new template: 444\n",
      "add a new template: 445\n",
      "add a new template: 446\n",
      "add a new template: 447\n",
      "add a new template: 448\n",
      "add a new template: 449\n",
      "add a new template: 450\n",
      "add a new template: 451\n",
      "add a new template: 452\n",
      "add a new template: 453\n",
      "add a new template: 454\n",
      "add a new template: 455\n",
      "add a new template: 456\n",
      "add a new template: 457\n",
      "add a new template: 458\n",
      "add a new template: 459\n",
      "add a new template: 460\n",
      "add a new template: 461\n",
      "add a new template: 462\n",
      "add a new template: 463\n",
      "add a new template: 464\n",
      "add a new template: 465\n",
      "add a new template: 466\n",
      "add a new template: 467\n",
      "add a new template: 468\n",
      "add a new template: 469\n",
      "add a new template: 470\n",
      "add a new template: 471\n",
      "add a new template: 472\n",
      "add a new template: 473\n",
      "add a new template: 474\n",
      "add a new template: 475\n",
      "add a new template: 476\n",
      "add a new template: 477\n",
      "add a new template: 478\n",
      "add a new template: 479\n",
      "add a new template: 480\n",
      "add a new template: 481\n",
      "add a new template: 482\n",
      "add a new template: 483\n",
      "add a new template: 484\n",
      "add a new template: 485\n",
      "add a new template: 486\n",
      "add a new template: 487\n",
      "add a new template: 488\n",
      "add a new template: 489\n",
      "add a new template: 490\n",
      "add a new template: 491\n",
      "add a new template: 492\n",
      "add a new template: 493\n",
      "add a new template: 494\n",
      "add a new template: 495\n",
      "add a new template: 496\n",
      "add a new template: 497\n",
      "add a new template: 498\n",
      "add a new template: 499\n",
      "add a new template: 500\n",
      "add a new template: 501\n",
      "add a new template: 502\n",
      "add a new template: 503\n",
      "add a new template: 504\n",
      "add a new template: 505\n",
      "add a new template: 506\n",
      "add a new template: 507\n",
      "add a new template: 508\n",
      "add a new template: 509\n",
      "add a new template: 510\n",
      "add a new template: 511\n",
      "add a new template: 512\n",
      "add a new template: 513\n",
      "add a new template: 514\n",
      "add a new template: 515\n",
      "add a new template: 516\n",
      "add a new template: 517\n",
      "add a new template: 518\n",
      "add a new template: 519\n",
      "add a new template: 520\n",
      "add a new template: 521\n",
      "add a new template: 522\n",
      "add a new template: 523\n",
      "add a new template: 524\n",
      "add a new template: 525\n",
      "add a new template: 526\n",
      "add a new template: 527\n",
      "add a new template: 528\n",
      "add a new template: 529\n",
      "add a new template: 530\n",
      "add a new template: 531\n",
      "add a new template: 532\n",
      "add a new template: 533\n",
      "add a new template: 534\n",
      "add a new template: 535\n",
      "add a new template: 536\n",
      "add a new template: 537\n",
      "add a new template: 538\n",
      "add a new template: 539\n",
      "add a new template: 540\n",
      "add a new template: 541\n",
      "add a new template: 542\n",
      "add a new template: 543\n",
      "add a new template: 544\n",
      "add a new template: 545\n",
      "add a new template: 546\n",
      "add a new template: 547\n",
      "add a new template: 548\n",
      "add a new template: 549\n",
      "add a new template: 550\n",
      "add a new template: 551\n",
      "add a new template: 552\n",
      "add a new template: 553\n",
      "add a new template: 554\n",
      "add a new template: 555\n",
      "add a new template: 556\n",
      "add a new template: 557\n",
      "add a new template: 558\n",
      "add a new template: 559\n",
      "add a new template: 560\n",
      "add a new template: 561\n",
      "add a new template: 562\n",
      "add a new template: 563\n",
      "add a new template: 564\n",
      "add a new template: 565\n",
      "add a new template: 566\n",
      "add a new template: 567\n",
      "add a new template: 568\n",
      "add a new template: 569\n",
      "add a new template: 570\n",
      "add a new template: 571\n",
      "add a new template: 572\n",
      "add a new template: 573\n",
      "add a new template: 574\n",
      "add a new template: 575\n",
      "add a new template: 576\n",
      "add a new template: 577\n",
      "add a new template: 578\n",
      "add a new template: 579\n",
      "add a new template: 580\n",
      "add a new template: 581\n",
      "add a new template: 582\n",
      "add a new template: 583\n",
      "add a new template: 584\n",
      "add a new template: 585\n",
      "add a new template: 586\n",
      "add a new template: 587\n",
      "add a new template: 588\n",
      "add a new template: 589\n",
      "add a new template: 590\n",
      "add a new template: 591\n",
      "add a new template: 592\n",
      "add a new template: 593\n",
      "add a new template: 594\n",
      "add a new template: 595\n",
      "add a new template: 596\n",
      "add a new template: 597\n",
      "add a new template: 598\n",
      "add a new template: 599\n",
      "add a new template: 600\n",
      "add a new template: 601\n",
      "add a new template: 602\n",
      "add a new template: 603\n",
      "add a new template: 604\n",
      "add a new template: 605\n",
      "add a new template: 606\n",
      "add a new template: 607\n",
      "add a new template: 608\n",
      "add a new template: 609\n",
      "add a new template: 610\n",
      "add a new template: 611\n",
      "add a new template: 612\n",
      "add a new template: 613\n",
      "add a new template: 614\n",
      "add a new template: 615\n",
      "add a new template: 616\n",
      "add a new template: 617\n",
      "add a new template: 618\n",
      "add a new template: 619\n",
      "add a new template: 620\n",
      "add a new template: 621\n",
      "add a new template: 622\n",
      "add a new template: 623\n",
      "add a new template: 624\n",
      "add a new template: 625\n",
      "add a new template: 626\n",
      "add a new template: 627\n",
      "add a new template: 628\n",
      "add a new template: 629\n",
      "add a new template: 630\n",
      "add a new template: 631\n",
      "add a new template: 632\n",
      "add a new template: 633\n",
      "add a new template: 634\n",
      "add a new template: 635\n",
      "add a new template: 636\n",
      "add a new template: 637\n",
      "add a new template: 638\n",
      "add a new template: 639\n",
      "add a new template: 640\n",
      "add a new template: 641\n",
      "add a new template: 642\n",
      "add a new template: 643\n",
      "add a new template: 644\n",
      "add a new template: 645\n",
      "add a new template: 646\n",
      "add a new template: 647\n",
      "add a new template: 648\n",
      "add a new template: 649\n",
      "add a new template: 650\n",
      "add a new template: 651\n",
      "add a new template: 652\n",
      "add a new template: 653\n",
      "add a new template: 654\n",
      "add a new template: 655\n",
      "add a new template: 656\n",
      "add a new template: 657\n",
      "add a new template: 658\n",
      "add a new template: 659\n",
      "add a new template: 660\n",
      "add a new template: 661\n",
      "add a new template: 662\n",
      "add a new template: 663\n",
      "add a new template: 664\n",
      "add a new template: 665\n",
      "add a new template: 666\n",
      "add a new template: 667\n",
      "add a new template: 668\n",
      "add a new template: 669\n",
      "add a new template: 670\n",
      "add a new template: 671\n",
      "add a new template: 672\n",
      "add a new template: 673\n",
      "add a new template: 674\n",
      "add a new template: 675\n",
      "add a new template: 676\n",
      "add a new template: 677\n",
      "add a new template: 678\n",
      "add a new template: 679\n",
      "add a new template: 680\n",
      "add a new template: 681\n",
      "add a new template: 682\n",
      "add a new template: 683\n",
      "add a new template: 684\n",
      "add a new template: 685\n",
      "add a new template: 686\n",
      "add a new template: 687\n",
      "add a new template: 688\n",
      "add a new template: 689\n",
      "add a new template: 690\n",
      "add a new template: 691\n",
      "add a new template: 692\n",
      "add a new template: 693\n",
      "add a new template: 694\n",
      "add a new template: 695\n",
      "add a new template: 696\n",
      "add a new template: 697\n",
      "add a new template: 698\n",
      "add a new template: 699\n",
      "add a new template: 700\n",
      "add a new template: 701\n",
      "add a new template: 702\n",
      "add a new template: 703\n",
      "add a new template: 704\n",
      "add a new template: 705\n",
      "add a new template: 706\n",
      "add a new template: 707\n",
      "add a new template: 708\n",
      "add a new template: 709\n",
      "add a new template: 710\n",
      "add a new template: 711\n",
      "add a new template: 712\n",
      "add a new template: 713\n",
      "add a new template: 714\n",
      "add a new template: 715\n",
      "add a new template: 716\n",
      "add a new template: 717\n",
      "add a new template: 718\n",
      "add a new template: 719\n",
      "add a new template: 720\n",
      "add a new template: 721\n",
      "add a new template: 722\n",
      "add a new template: 723\n",
      "add a new template: 724\n",
      "add a new template: 725\n",
      "add a new template: 726\n",
      "add a new template: 727\n",
      "add a new template: 728\n",
      "add a new template: 729\n",
      "add a new template: 730\n",
      "add a new template: 731\n",
      "add a new template: 732\n",
      "add a new template: 733\n",
      "add a new template: 734\n",
      "add a new template: 735\n",
      "add a new template: 736\n",
      "add a new template: 737\n",
      "add a new template: 738\n",
      "add a new template: 739\n",
      "add a new template: 740\n",
      "add a new template: 741\n",
      "add a new template: 742\n",
      "add a new template: 743\n",
      "add a new template: 744\n",
      "add a new template: 745\n",
      "add a new template: 746\n",
      "add a new template: 747\n",
      "add a new template: 748\n",
      "add a new template: 749\n",
      "add a new template: 750\n",
      "add a new template: 751\n",
      "add a new template: 752\n",
      "add a new template: 753\n",
      "add a new template: 754\n",
      "add a new template: 755\n",
      "add a new template: 756\n",
      "add a new template: 757\n",
      "add a new template: 758\n",
      "add a new template: 759\n",
      "add a new template: 760\n",
      "add a new template: 761\n",
      "add a new template: 762\n",
      "add a new template: 763\n",
      "add a new template: 764\n",
      "add a new template: 765\n",
      "add a new template: 766\n",
      "add a new template: 767\n",
      "add a new template: 768\n",
      "add a new template: 769\n",
      "add a new template: 770\n",
      "add a new template: 771\n",
      "add a new template: 772\n",
      "add a new template: 773\n",
      "add a new template: 774\n",
      "add a new template: 775\n",
      "add a new template: 776\n",
      "add a new template: 777\n",
      "add a new template: 778\n",
      "add a new template: 779\n",
      "add a new template: 780\n",
      "add a new template: 781\n",
      "add a new template: 782\n",
      "add a new template: 783\n",
      "add a new template: 784\n",
      "add a new template: 785\n",
      "add a new template: 786\n",
      "add a new template: 787\n",
      "add a new template: 788\n",
      "add a new template: 789\n",
      "add a new template: 790\n",
      "add a new template: 791\n",
      "add a new template: 792\n",
      "add a new template: 793\n",
      "add a new template: 794\n",
      "add a new template: 795\n",
      "add a new template: 796\n",
      "add a new template: 797\n",
      "add a new template: 798\n",
      "add a new template: 799\n",
      "add a new template: 800\n",
      "add a new template: 801\n",
      "add a new template: 802\n",
      "add a new template: 803\n",
      "add a new template: 804\n",
      "add a new template: 805\n",
      "add a new template: 806\n",
      "add a new template: 807\n",
      "add a new template: 808\n",
      "add a new template: 809\n",
      "add a new template: 810\n",
      "add a new template: 811\n",
      "add a new template: 812\n",
      "add a new template: 813\n",
      "add a new template: 814\n",
      "add a new template: 815\n",
      "add a new template: 816\n",
      "add a new template: 817\n",
      "add a new template: 818\n",
      "add a new template: 819\n",
      "add a new template: 820\n",
      "add a new template: 821\n",
      "add a new template: 822\n",
      "add a new template: 823\n",
      "add a new template: 824\n",
      "add a new template: 825\n",
      "add a new template: 826\n",
      "add a new template: 827\n",
      "add a new template: 828\n",
      "add a new template: 829\n",
      "add a new template: 830\n",
      "add a new template: 831\n",
      "add a new template: 832\n",
      "add a new template: 833\n",
      "add a new template: 834\n",
      "add a new template: 835\n",
      "add a new template: 836\n",
      "add a new template: 837\n",
      "add a new template: 838\n",
      "add a new template: 839\n",
      "add a new template: 840\n",
      "add a new template: 841\n",
      "add a new template: 842\n",
      "add a new template: 843\n",
      "add a new template: 844\n",
      "add a new template: 845\n",
      "add a new template: 846\n",
      "add a new template: 847\n",
      "add a new template: 848\n",
      "add a new template: 849\n",
      "add a new template: 850\n",
      "add a new template: 851\n",
      "add a new template: 852\n",
      "add a new template: 853\n",
      "add a new template: 854\n",
      "add a new template: 855\n",
      "add a new template: 856\n",
      "add a new template: 857\n",
      "add a new template: 858\n",
      "add a new template: 859\n",
      "add a new template: 860\n",
      "add a new template: 861\n",
      "add a new template: 862\n",
      "add a new template: 863\n",
      "add a new template: 864\n",
      "add a new template: 865\n",
      "add a new template: 866\n",
      "add a new template: 867\n",
      "add a new template: 868\n",
      "add a new template: 869\n",
      "add a new template: 870\n",
      "add a new template: 871\n",
      "add a new template: 872\n",
      "add a new template: 873\n",
      "add a new template: 874\n",
      "add a new template: 875\n",
      "add a new template: 876\n",
      "add a new template: 877\n",
      "add a new template: 878\n",
      "add a new template: 879\n",
      "add a new template: 880\n",
      "add a new template: 881\n",
      "add a new template: 882\n",
      "add a new template: 883\n",
      "add a new template: 884\n",
      "add a new template: 885\n",
      "add a new template: 886\n",
      "add a new template: 887\n",
      "add a new template: 888\n",
      "add a new template: 889\n",
      "add a new template: 890\n",
      "add a new template: 891\n",
      "add a new template: 892\n",
      "add a new template: 893\n",
      "add a new template: 894\n",
      "add a new template: 895\n",
      "add a new template: 896\n",
      "add a new template: 897\n",
      "add a new template: 898\n",
      "add a new template: 899\n",
      "add a new template: 900\n",
      "add a new template: 901\n",
      "add a new template: 902\n",
      "add a new template: 903\n",
      "add a new template: 904\n",
      "add a new template: 905\n",
      "add a new template: 906\n",
      "add a new template: 907\n",
      "add a new template: 908\n",
      "add a new template: 909\n",
      "add a new template: 910\n",
      "add a new template: 911\n",
      "add a new template: 912\n",
      "add a new template: 913\n",
      "add a new template: 914\n",
      "add a new template: 915\n",
      "add a new template: 916\n",
      "add a new template: 917\n",
      "add a new template: 918\n",
      "add a new template: 919\n",
      "add a new template: 920\n",
      "add a new template: 921\n",
      "add a new template: 922\n",
      "add a new template: 923\n",
      "add a new template: 924\n",
      "add a new template: 925\n",
      "add a new template: 926\n",
      "add a new template: 927\n",
      "add a new template: 928\n",
      "add a new template: 929\n",
      "add a new template: 930\n",
      "add a new template: 931\n",
      "add a new template: 932\n",
      "add a new template: 933\n",
      "add a new template: 934\n",
      "add a new template: 935\n",
      "add a new template: 936\n",
      "add a new template: 937\n",
      "add a new template: 938\n",
      "add a new template: 939\n",
      "add a new template: 940\n",
      "add a new template: 941\n",
      "add a new template: 942\n",
      "add a new template: 943\n",
      "add a new template: 944\n",
      "944\n",
      "processing samples...\n",
      "length of training set: 46419\n",
      "length of training set: 5207\n",
      "length of training set: 4030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caesarchen\\AppData\\Local\\Temp\\ipykernel_37432\\3045849123.py:147: UserWarning: Failed to initialize NumPy: DLL load failed while importing _multiarray_umath: 找不到指定的模块。 (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  vec = torch.tensor([float(x) for x in parts[1:]], dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "tags_file = \"atis-fields.txt\"        \n",
    "data_file = \"atis.json\"\n",
    "type_file = \"atis-schema.csv\"     \n",
    "glove_path = \"glove.6B.50d.txt\"  \n",
    "    \n",
    "processor = atisDataProcessor(data_file, type_file, glove_path, type_emb_dim=50, dtype_emb_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78d854-230e-4a14-a7c9-4f9b1eaeec48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize linear model...\n",
      "===============Training Model=================\n"
     ]
    }
   ],
   "source": [
    "model_set = [\"linear\", \"feedforward\", \"lstm\", \"transformer\"]\n",
    "for model_type in model_set:\n",
    "    model = ClassificationModels(embedding_matrix=processor.embedding_matrix,\n",
    "                           type_vocab_size=len(processor.var2idx),\n",
    "                           dtype_vocab_size=len(processor.dtype2idx),\n",
    "                           type_emb_dim=50, dtype_emb_dim=50,\n",
    "                           model_type=model_type,\n",
    "                           hidden_dim=128,\n",
    "                           template_classes=processor.template_classes,\n",
    "                           tag_classes=len(processor.name2idx),\n",
    "                           num_layers=1, nhead=5)\n",
    "    print(f\"===============Training Model=================\")\n",
    "    train_model(processor, model, epochs=20, lr=1e-3, patience=3)\n",
    "    print(f\"===============Testing Model=================\")\n",
    "    acc_cls, acc_tag = evaluate_model(processor, model)\n",
    "    print(f\"Test set {model_type} —  Classification Acc: {acc_cls:.4f},  Tagging Acc: {acc_tag:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67693b2a-ce11-4a88-be3f-c4d6b32ab3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"show me the flights arriving at MKE\"\n",
    "pred_id, vars_detected = inference(model, processor, question)\n",
    "print(f\"template ID: {pred_id}\")\n",
    "print(f\"variables: {vars_detected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdecf8-236d-4278-b1b6-895b1afd2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM seq2seq\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, emb_dim=100, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        # 输入和输出的词嵌入层\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, emb_dim)\n",
    "        self.decoder = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Encoder\n",
    "        src_emb = self.encoder_embedding(src)\n",
    "        _, (hidden, cell) = self.encoder(src_emb)\n",
    "\n",
    "        # Decoder\n",
    "        tgt_emb = self.decoder_embedding(tgt)\n",
    "        output, _ = self.decoder(tgt_emb, (hidden, cell))\n",
    "        logits = self.out(output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Attention LSTM seq2seq\n",
    "class Seq2SeqLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, emb_dim=100, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, emb_dim)\n",
    "        self.decoder = nn.LSTM(emb_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.attn = nn.Linear(hidden_dim + emb_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Input\n",
    "        src_emb = self.encoder_embedding(src)\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src_emb)\n",
    "\n",
    "        # Attention\n",
    "        tgt_emb = self.decoder_embedding(tgt)\n",
    "        outputs = []\n",
    "        for t in range(tgt.size(1)):\n",
    "            emb_t = tgt_emb[:, t:t+1, :]  # 当前时刻的输入\n",
    "            # 计算注意力权重\n",
    "            attn_weights = torch.bmm(emb_t, encoder_outputs.transpose(1, 2))\n",
    "            attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "            context = torch.bmm(attn_weights, encoder_outputs)  # 加权求和\n",
    "            rnn_input = torch.cat((emb_t, context), dim=-1)  \n",
    "            output, (hidden, cell) = self.decoder(rnn_input, (hidden, cell))\n",
    "            logits = self.out(output)  # 输出词预测结果\n",
    "            outputs.append(logits)\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "# Transformer\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, emb_dim=256, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        # 词嵌入层\n",
    "        self.src_embedding = nn.Embedding(input_vocab_size, emb_dim)\n",
    "        self.tgt_embedding = nn.Embedding(output_vocab_size, emb_dim)\n",
    "\n",
    "        # Transformer \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(emb_dim, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.src_embedding(src)\n",
    "        tgt_emb = self.tgt_embedding(tgt)\n",
    "        memory = self.transformer.encoder(src_emb)  # 编码输入序列\n",
    "        output = self.transformer.decoder(tgt_emb, memory)  # 解码目标序列\n",
    "        return self.out(output)  # 输出词预测结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLPCodingTest]",
   "language": "python",
   "name": "conda-env-NLPCodingTest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
